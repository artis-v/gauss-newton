\documentclass[12pt, a4paper]{article}
\usepackage[a4paper, margin=3.9cm]{geometry}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amsfonts}

\usepackage{fancyvrb}

\usepackage{listingsutf8}
\usepackage[apple]{modernlistings} % Can be found at https://gist.github.com/maelquerre/b625bda6cab11c1b0ae67a2be86f027e

\setlength\parindent{0pt}
\setlength\parskip{10pt}

\newcommand{\bx}{{\boldsymbol{x}}}
\newcommand{\bd}{{\boldsymbol{\delta}}}

\title{Project report:
\\ Gauss-Newton optimization method}

\author{Nicolas Munke Cilano
\\ Vidar Gimbringer 
\\ Artis Vijups 
\vspace{20pt} \\ Supervisor: Stefan Diehl}
% OBS: Don't publish personal numbers to GitHub!

\begin{document}

\maketitle

\section{Introduction}

In this project, we mainly investigate the problem of fitting the function \[\varphi(\bx; t)=x_1e^{-x_2t}+x_3e^{-x_4t},\] where $\bx={(x_1,x_2,x_3,x_4)}^{T}$ are parameters, to a given set of data points $(t_i,y_i), i=1,\ldots,m$.

In particular, we seek to minimize the sum of the squared distances from each data point $(t_i,y_i)$ to the point $(t_i,\varphi(\bx; t_i))$. If we denote the distance by $r_i(\bx)=\varphi(\bx; t_i)-y_i$, then our goal is to \[\underset{\bx\in\mathbb{R}^4}{\text{minimize}}~f(\bx)\quad\text{where}\quad f(\bx)=\sum_{i=1}^{m}{r_i(\bx)}^2.\]

To solve this minimization problem, we use the Gauss-Newton method.

\section{Methods}

\subsection{Gauss-Newton method}

Let $r(\bx)={(r_1(\bx),\ldots,r_m(\bx))}^T$. The method is set up on the idea that \[r(\bx+\bd)\approx r(\bx)+J(\bx)\bd,\]where $J(\bx)$ is the Jacobian matrix of $r(\bx)$ and $\bd$ is an increment vector, also a direction vector.

This can be thought of as an extension to more dimensions of the strategy of moving along the tangent line to estimate a nearby value.

We then observe that \begin{align*}
f(\bx+\bd) &= \sum_{i=1}^{m}{r_i(\bx+\bd)}^2 \\ 
&= {r(\bx+\bd)}^T r(\bx+\bd) \\
&\approx {(r(\bx)+J(\bx)\bd)}^T (r(\bx)+J(\bx)\bd).
\end{align*}

Since we want the output of $f $ to be the minimum possible, the gradient of this approximation should be $0$. Writing that as an equation and simplifying, we end up with \[{J(\bx)}^T J(\bx)\bd=-{J(\bx)}^T r(\bx).\]

This presents the following iterative algorithm:
\begin{enumerate}
    \item Solve the linear system ${J(\bx)}^T J(\bx)\bd=-{J(\bx)}^T r(\bx)$ for $\bd$.
    \item Determine an optimal step length $\lambda$ for direction $\bd$ using a line search algorithm.
    \item Update $\bx$ to $\bx+\lambda\bd$.
\end{enumerate}
We repeat these actions until the step $\lambda\bd$ is smaller than our chosen tolerance.

\subsection{Line search algorithm}

For the line search mentioned in the second step of the Gauss-Newton iteration algorithm, we use Armijo's rule on $F(\lambda)=f(\bx+\lambda\bd)$.

Let $T(\lambda)=F(0)+\varepsilon F'(0)\lambda$ be a straight line through $(0,F(0))$ with less negative slope than the point's tangent, so $0<\varepsilon<1$.

Armijo's rule is made up of two (upper and lower) conditions, which are\[F(\lambda)\le T(\lambda)\quad\text{and}\quad F(\alpha\lambda)\ge T(\alpha\lambda)\text{ for fixed }\alpha>1.\]
This rule ensures $\lambda$ will be in an \textit{interval} of points where $F $ is substantially smaller. Computationally, this is faster than looking for a perfect choice for $\lambda$.

So that $\lambda$ satisfies Armijo's rule, we choose it as follows:\begin{enumerate}
    \item Make an initial guess for $\lambda$.
    \item Repeatedly scale $\lambda$ up by $\alpha$ until it satisfies the lower condition.
    \item Repeatedly scale $\lambda$ down by $\alpha$ until it satisfies the upper condition. 
\end{enumerate}

% !!
% This subsection should further motivate why Armijo was chosen.
% !!

\section{Project work}

\subsection{Responsibilities}

Nicolas ...

Vidar ...

Artis ...

\subsection{Structure}

The project is stored on GitHub as a repository. It is made up of:\begin{itemize}
    \item phi1.m, phi2.m, data1.m, data2.m, grad.m as provided,
    \item gaussnewton.m, the implementation of the Gauss-Newton method,
    \item line\_search.m, the line search algorithm based on Armijo's rule,
    \item script.m, the main script containing tests and tasks,
    \item report.tex and report.pdf, forming this report,
    \item metadata files.
\end{itemize}

\subsection{Results}

We fit the two functions from phi1.m and phi2.m, \[\varphi_1(\bx;t)=x_1e^{-x_2t},\quad\bx={(x_1,x_2)}^T\] and \[\varphi_2(\bx;t)=\varphi(\bx; t)=x_1e^{-x_2t}+x_3e^{-x_4t},\quad\bx={(x_1,x_2,x_3,x_4)}^T,\] to two sets of data points, \texttt{d1} and \texttt{d2} from data1.m and data2.m.

The results are:
\begin{center}\bgroup\def\arraystretch{1.4}
\begin{tabular}{|l|l|l|l|}
    \hline
    Case & Optimal point $\bx$ & $\Vert \nabla f(\bx) \Vert_2$ & $\Vert r(\bx) \Vert_{\infty}$
    \\ \hline
    \texttt{d1}, $\varphi_1$ & ${(10.8108, 2.4786)}^T$ & $3.7450\cdot10^{-4}$ & $1.6287$
    \\ \hline
    \texttt{d2}, $\varphi_1$ & ${(12.9789, 1.7861)}^T$ & $8.0031\cdot10^{-2}$ & $1.0397$
    \\ \hline
    \texttt{d1}, $\varphi_2$ & ${(6.3445, 10.5866, 6.0959, 1.4003)}^T$ & $4.0651\cdot10^{-5}$ & $0.4334$
    \\ \hline
    \texttt{d2}, $\varphi_2$ & ${(4.1741, 0.8747, 9.7390, 2.9208)}^T$ & $9.5220\cdot10^{-3}$ & $0.1182$
    \\ \hline
\end{tabular}
\egroup\end{center}

\subsection{Initial points}

% A table and short discussion on how the convergence 
% (total number of iterations and function evaluations 
% for a given tolerance) and the solution are affected 
% by different initial points.

% A strategy to choose the initial point for phi2.m.

\subsection{Detailed result}

For the \texttt{d2}, $\varphi_2$ case, we present a full printout. The initial point for this example is ${(1,2,3,4)}^T$.
\scriptsize\begin{Verbatim}[frame=single]
  Gauss-Newton Iteration 1: Starting line search...
    Line Search Backtrack Iteration: lambda = 5.0000e-01, F(lambda) = 2.8163e+22
    Line Search Backtrack Iteration: lambda = 2.5000e-01, F(lambda) = 8.4145e+10
    Line Search Backtrack Iteration: lambda = 1.2500e-01, F(lambda) = 1.5305e+05
    Line Search Backtrack Iteration: lambda = 6.2500e-02, F(lambda) = 2.7764e+04
  Gauss-Newton Iteration 1:
  x = [-0.1860, 0.3882, 4.7977, 3.2363]
  max(abs(r)) = 1.0019e+01, norm(grad) = 6.8024e+03, lambda = 6.2500e-02
  Gauss-Newton Iteration 2: Starting line search...
    Line Search Backtrack Iteration: lambda = 5.0000e-01, F(lambda) = 2.7633e+06
    Line Search Backtrack Iteration: lambda = 2.5000e-01, F(lambda) = 1.4590e+04
  Gauss-Newton Iteration 2:
  x = [0.7098, -0.8628, 6.2213, 2.9969]
  max(abs(r)) = 9.4068e+00, norm(grad) = 1.1161e+04, lambda = 2.5000e-01
  Gauss-Newton Iteration 3: Starting line search...
  Gauss-Newton Iteration 3:
  x = [0.8378, -0.3380, 12.8793, 1.4757]
  max(abs(r)) = 7.0874e+00, norm(grad) = 1.5113e+04, lambda = 1.0000e+00
  Gauss-Newton Iteration 4: Starting line search...
  Gauss-Newton Iteration 4:
  x = [3.4844, 1.2798, 10.2036, 2.5278]
  max(abs(r)) = 2.3072e+00, norm(grad) = 2.1603e+04, lambda = 1.0000e+00
  Gauss-Newton Iteration 5: Starting line search...
  Gauss-Newton Iteration 5:
  x = [1.6255, 0.2138, 12.2776, 2.7719]
  max(abs(r)) = 6.2761e-01, norm(grad) = 1.5974e+03, lambda = 1.0000e+00
  Gauss-Newton Iteration 6: Starting line search...
  Gauss-Newton Iteration 6:
  x = [2.6973, 0.7203, 11.1893, 2.7070]
  max(abs(r)) = 5.4273e-01, norm(grad) = 9.6006e+02, lambda = 1.0000e+00
  Gauss-Newton Iteration 7: Starting line search...
    Line Search Iteration: lambda = 2.0000e+00, F(lambda) = 2.0939e+01
  Gauss-Newton Iteration 7:
  x = [5.5035, 1.1028, 8.4286, 3.0652]
  max(abs(r)) = 3.2980e-01, norm(grad) = 8.9192e+02, lambda = 2.0000e+00
  Gauss-Newton Iteration 8: Starting line search...
  Gauss-Newton Iteration 8:
  x = [3.7177, 0.8468, 10.1922, 2.8388]
  max(abs(r)) = 2.4740e-01, norm(grad) = 2.7354e+02, lambda = 1.0000e+00
  Gauss-Newton Iteration 9: Starting line search...
  Gauss-Newton Iteration 9:
  x = [4.1808, 0.8779, 9.7318, 2.9178]
  max(abs(r)) = 1.9147e-01, norm(grad) = 3.4489e+02, lambda = 1.0000e+00
  Gauss-Newton Iteration 10: Starting line search...
  Gauss-Newton Iteration 10:
  x = [4.1741, 0.8747, 9.7390, 2.9208]
  max(abs(r)) = 1.2141e-01, norm(grad) = 7.5698e+00, lambda = 1.0000e+00
  Gauss-Newton Iteration 11: Starting line search...
    Line Search Iteration: lambda = 2.0000e+00, F(lambda) = 8.9616e+00
  Gauss-Newton Iteration 11: Converged with tol 1.0000e-04, max(abs(r)) 1.1817e-01.
  
  Final Results:
  x = [4.1741, 0.8747, 9.7390, 2.9208]
  Final norm(grad) = 9.5220e-03
  Total iterations = 11
  Total function evaluations = 30
  Total elapsed time: 0.2712 seconds
\end{Verbatim}

\pagebreak

\pagenumbering{gobble}

\section*{Appendix}

\subsection*{gaussnewton.m}
\lstinputlisting[language=Matlab,
numbers=none, 
xleftmargin=0pt,
numbersep=0pt,
basicstyle=\ttfamily\scriptsize]{gaussnewton.m}

\pagebreak

\subsection*{line\_search.m}
\lstinputlisting[language=Matlab,
numbers=none, 
xleftmargin=0pt,
numbersep=0pt, 
basicstyle=\ttfamily\scriptsize]{line_search.m}

\pagebreak

\subsection*{script.m}
\lstinputlisting[language=Matlab,
numbers=none, 
xleftmargin=0pt,
numbersep=0pt, 
basicstyle=\ttfamily\scriptsize]{script.m}

\end{document}